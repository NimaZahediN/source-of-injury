{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration File Details:**\n",
    "\n",
    "- **Tokenizer Type:** LlamaTokenizer\n",
    "- **Special Tokens:**\n",
    "  - `bos_token`: \"~~\"\n",
    "  - `eos_token`: \"~~\"\n",
    "  - `unk_token`: \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the tokenizer is correctly initialized with the specified special tokens:\n",
    "from transformers import AutoTokenizer\n",
    "   \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NimaZahedinameghi/source_of_injury\")\n",
    "assert tokenizer.bos_token == \"~~\"\n",
    "assert tokenizer.eos_token == \"~~\"\n",
    "assert tokenizer.unk_token == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"IncidentDescription: While working on a vehicle repair, I had to contort my body...\"\n",
    "tokens = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that special tokens are correctly added to the tokenizer configuration and used consistently:\n",
    "assert tokenizer.special_tokens_map[\"bos_token\"] == \"~~\"\n",
    "assert tokenizer.special_tokens_map[\"eos_token\"] == \"~~\"\n",
    "assert tokenizer.special_tokens_map[\"unk_token\"] == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Check the processed dataset to ensure tokenization aligns with the special tokens and format:   \n",
    "from datasets import load_from_disk\n",
    "   \n",
    "dataset = load_from_disk('last_run_prepared')\n",
    "for sample in dataset['train']:\n",
    "    tokens = tokenizer.decode(sample['input_ids'])\n",
    "    assert tokens.startswith(\"~~\")\n",
    "    assert tokens.endswith(\"~~\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
